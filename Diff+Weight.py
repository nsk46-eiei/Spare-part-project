import pandas as pd
import numpy as np
import datetime
import random
import warnings

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Library
try:
    from sklearn.neural_network import MLPRegressor
    from sklearn.preprocessing import MinMaxScaler
except ImportError:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö sklearn! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå 'pip install scikit-learn' ‡πÉ‡∏ô Terminal")
    exit()

warnings.filterwarnings('ignore')

# --- ‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢) ---
# 1.0 = ‡πÄ‡∏ó‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á (‡∏ô‡πâ‡∏≠‡∏¢), 3.0 = ‡πÄ‡∏¢‡∏≠‡∏∞‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° 3 ‡πÄ‡∏ó‡πà‡∏≤, 5.0 = ‡πÄ‡∏¢‡∏≠‡∏∞‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° 5 ‡πÄ‡∏ó‡πà‡∏≤
# ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ 3.0 - 5.0 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥ ML ‡∏Ñ‡∏£‡∏±‡∏ö
DATA_MULTIPLIER = 4.0 

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
file_name = 'data.xlsx'
WINDOW_SIZE = 15   
STEPS = 30         

print(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Diffusion ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML (Target Multiplier: x{DATA_MULTIPLIER})...")

try:
    # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
    df = pd.read_excel(file_name)
    df.columns = [str(c).strip() for c in df.columns]
    
    date_col = 'requisition date'
    qty_col = '#Requisition'
    if date_col not in df.columns: date_col = df.columns[6]
    if qty_col not in df.columns: qty_col = df.columns[7]

    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
    df[qty_col] = pd.to_numeric(df[qty_col], errors='coerce').abs()
    
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà (Weight)
    print("üì¶ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤...")
    detail_columns = [c for c in df.columns if c not in [date_col, qty_col, 'Data Type']]
    unique_df = df[detail_columns].drop_duplicates()
    population = unique_df.to_dict('records')
    
    id_col = df.columns[0]
    freq_map = df[id_col].value_counts().to_dict()
    weights = [freq_map.get(item[id_col], 1) for item in population]

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≠‡∏ô AI
    # ‡πÉ‡∏ä‡πâ .size() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏ö Transaction ‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô (Pattern ‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏á‡∏≤‡∏ô)
    df_daily = df.groupby(date_col).size().resample('D').sum().fillna(0) 
    data_values = df_daily.values.reshape(-1, 1)

    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(data_values).flatten()

    # --- 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Training Data ---
    X_train_seq = []
    for i in range(len(data_scaled) - WINDOW_SIZE):
        X_train_seq.append(data_scaled[i : i + WINDOW_SIZE])
    X_train_seq = np.array(X_train_seq)

    # --- 3. Diffusion Parameters ---
    betas = np.linspace(0.0001, 0.02, STEPS)
    alphas = 1.0 - betas
    alphas_cumprod = np.cumprod(alphas)
    sqrt_alphas_cumprod = np.sqrt(alphas_cumprod)
    sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)

    # --- 4. ‡∏™‡∏≠‡∏ô Model ---
    print("ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Pattern ‡∏Å‡∏≤‡∏£‡πÄ‡∏ö‡∏¥‡∏Å‡∏à‡πà‡∏≤‡∏¢ (Training)...")
    X_input = []
    y_target = []

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô (Epochs) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏à‡∏≥ Pattern ‡πÅ‡∏°‡πà‡∏ô‡πÜ
    for _ in range(10): 
        for i in range(len(X_train_seq)):
            t = np.random.randint(0, STEPS)
            noise = np.random.randn(WINDOW_SIZE)
            clean_seq = X_train_seq[i]
            noisy_seq = (sqrt_alphas_cumprod[t] * clean_seq) + \
                        (sqrt_one_minus_alphas_cumprod[t] * noise)
            
            input_vector = np.append(noisy_seq, t / STEPS)
            X_input.append(input_vector)
            y_target.append(noise)

    model = MLPRegressor(hidden_layer_sizes=(128, 128), max_iter=500, random_state=42)
    model.fit(X_input, y_target)
    print("‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!")

    # --- 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (Generation) ---
    first_date_real = df[date_col].min()
    start_back_date = pd.Timestamp('2020-01-01')
    days_needed = (first_date_real - start_back_date).days
    
    print(f"‚ú® ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏ö‡∏ö High-Density ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML ({days_needed} ‡∏ß‡∏±‡∏ô)...")
    
    num_segments = (days_needed // WINDOW_SIZE) + 1
    synthetic_counts_seq = []

    for seg in range(num_segments):
        current_seq = np.random.randn(WINDOW_SIZE)
        for t in reversed(range(STEPS)):
            input_vector = np.append(current_seq, t / STEPS).reshape(1, -1)
            predicted_noise = model.predict(input_vector)[0]
            
            alpha = alphas[t]
            alpha_cumprod = alphas_cumprod[t]
            beta = betas[t]
            noise_z = np.random.randn(WINDOW_SIZE) if t > 0 else 0
            
            current_seq = (1 / np.sqrt(alpha)) * (current_seq - ((1 - alpha) / (np.sqrt(1 - alpha_cumprod))) * predicted_noise) + (np.sqrt(beta) * noise_z)
        synthetic_counts_seq.extend(current_seq)

    synthetic_counts_seq = synthetic_counts_seq[:days_needed]
    synthetic_counts_scaled = np.array(synthetic_counts_seq).reshape(-1, 1)
    synthetic_counts_raw = scaler.inverse_transform(synthetic_counts_scaled).flatten()
    
    # --- [‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç] Calibration ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πà‡∏á‡∏¢‡∏≠‡∏î (Augmentation) ---
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏¢‡∏≠‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö "‡∏ó‡∏ß‡∏µ‡∏Ñ‡∏π‡∏ì" (Multiplier)
    avg_rows_per_day = len(df) / ((df[date_col].max() - df[date_col].min()).days + 1)
    target_total = int(avg_rows_per_day * days_needed * DATA_MULTIPLIER)
    
    current_sum = np.sum(np.abs(synthetic_counts_raw))
    scaling_factor = target_total / current_sum if current_sum != 0 else 1
    
    # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô (Upscaling)
    synthetic_counts = np.floor(np.abs(synthetic_counts_raw) * scaling_factor).astype(int)
    
    # ‡πÄ‡∏ï‡∏¥‡∏° Randomness ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ô‡∏¥‡πà‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏î‡∏µ‡∏ï‡πà‡∏≠ ML)
    synthetic_counts = synthetic_counts + np.random.choice([0, 1], size=len(synthetic_counts), p=[0.7, 0.3])

    print(f"üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á: {np.sum(synthetic_counts)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ x{DATA_MULTIPLIER})")
    print(f"   (‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÑ‡∏õ‡∏ó‡∏≥ ML Training ‡∏Ñ‡∏£‡∏±‡∏ö)")

    # --- 6. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ---
    synthetic_data = []
    current_date = start_back_date
    avg_qty_per_job = df[qty_col].mean()

    for daily_job_count in synthetic_counts:
        if daily_job_count > 0:
            # ‡πÉ‡∏ä‡πâ Weighted Random Choice ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏ß‡πà‡∏≤‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÑ‡∏´‡∏ô‡∏Æ‡∏¥‡∏ï)
            selected_items = random.choices(population, weights=weights, k=daily_job_count)
            for item in selected_items:
                new_row = item.copy()
                new_row[date_col] = current_date
                
                # Variation ‡∏Ç‡∏≠‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ä‡∏¥‡πâ‡∏ô (‡πÉ‡∏´‡πâ ML ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢)
                noise_qty = np.random.uniform(0.5, 1.8) # ‡πÅ‡∏Å‡∏ß‡πà‡∏á‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡∏¥‡∏î‡∏ô‡∏∂‡∏á
                sim_qty = max(1, int(avg_qty_per_job * noise_qty))
                new_row[qty_col] = sim_qty
                
                new_row['Data Type'] = 'Synthetic (ML Augmented)'
                synthetic_data.append(new_row)
        
        current_date += datetime.timedelta(days=1)

    # --- 7. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ---
    if synthetic_data:
        df_past = pd.DataFrame(synthetic_data)
        df['Data Type'] = 'Actual'
        df_final = pd.concat([df_past, df], ignore_index=True)
        
        output_filename = "ML_Ready_Sparepart_Data.xlsx"
        df_final.sort_values(by=date_col).to_excel(output_filename, index=False)
        print(f"\nüéâ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥ ML ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö: {output_filename}")
        print(f"Total Rows: {len(df_final)}")
    else:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á)")

except Exception as e:
    print(f"\n‚ùå Error: {e}")